{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ðŸ—ºï¸ Preprocessing Absensi - Deteksi Anomali Lokasi\n",
    "**Pipeline:** Raw Data â†’ Time Transform â†’ Coordinate Transform â†’ DBSCAN â†’ Centroid Kantor â†’ Haversine â†’ Validation â†’ ST-DBSCAN â†’ Anomaly Scoring â†’ Risk Level\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn openpyxl haversine folium streamlit pydeck plotly streamlit-folium -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import_md",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from haversine import haversine, Unit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('âœ… Libraries loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_md",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GANTI PATH SESUAI FILE ANDA\n",
    "# Jika di Google Colab, upload file dulu atau mount Google Drive\n",
    "# ============================================================\n",
    "\n",
    "# Option 1: Upload langsung di Colab\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# FILE_PATH = list(uploaded.keys())[0]\n",
    "\n",
    "# Option 2: Mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# FILE_PATH = '/content/drive/MyDrive/dataset_absensi_final2.xlsx'\n",
    "\n",
    "# Option 3: Path lokal\n",
    "FILE_PATH = 'dataset_absensi_final2.xlsx'\n",
    "\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "print(f'âœ… Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns')\n",
    "print(f'Columns: {df.columns.tolist()}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda_md",
   "metadata": {},
   "source": [
    "## ðŸ” Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== INFO ===')\n",
    "df.info()\n",
    "print('\\n=== MISSING VALUES ===')\n",
    "print(df.isnull().sum())\n",
    "print('\\n=== JENIS ABSENSI ===')\n",
    "print(df['jenis'].value_counts())\n",
    "print('\\n=== STATUS LOKASI ===')\n",
    "print(df['status_lokasi'].value_counts())\n",
    "print('\\n=== JUMLAH SKPD ===')\n",
    "print(f\"Unique SKPD: {df['id_skpd'].nunique()}\")\n",
    "print(f\"Unique Karyawan: {df['karyawan_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1_md",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŸ¡ STEP 1: Time Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time_transform",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Membuat feature waktu dari kolom tanggal_kirim:\n",
    "    - timestamp_num : detik sejak epoch\n",
    "    - jam            : jam (0-23)\n",
    "    - menit          : menit (0-59)\n",
    "    - jam_desimal    : jam + menit/60\n",
    "    - weekday        : hari dalam minggu (0=Senin, 6=Minggu)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Pastikan datetime\n",
    "    df['tanggal_kirim'] = pd.to_datetime(df['tanggal_kirim'])\n",
    "    \n",
    "    # Timestamp numerik (detik sejak epoch)\n",
    "    df['timestamp_num'] = df['tanggal_kirim'].astype(np.int64) // 10**9\n",
    "    \n",
    "    # Ekstraksi komponen waktu\n",
    "    df['jam']         = df['tanggal_kirim'].dt.hour\n",
    "    df['menit']       = df['tanggal_kirim'].dt.minute\n",
    "    df['jam_desimal'] = df['jam'] + df['menit'] / 60.0\n",
    "    df['weekday']     = df['tanggal_kirim'].dt.weekday  # 0=Senin\n",
    "    df['tanggal']     = df['tanggal_kirim'].dt.date\n",
    "    \n",
    "    print('âœ… Time features created: timestamp_num, jam, menit, jam_desimal, weekday, tanggal')\n",
    "    return df\n",
    "\n",
    "df = transform_time_features(df)\n",
    "df[['tanggal_kirim','timestamp_num','jam','menit','jam_desimal','weekday']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2_md",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŸ  STEP 2: Coordinate Transformation (Radian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coord_transform",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_coordinates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Konversi lat/long ke radian untuk DBSCAN metric haversine.\n",
    "    Juga bersihkan koordinat yang tidak valid.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Drop baris dengan koordinat null\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=['lat', 'long'])\n",
    "    after = len(df)\n",
    "    if before != after:\n",
    "        print(f'âš ï¸  Dropped {before - after} rows with null coordinates')\n",
    "    \n",
    "    # Filter koordinat tidak valid (di luar range Indonesia)\n",
    "    # Indonesia: lat -11 s/d 6, long 95 s/d 141\n",
    "    mask_valid = (\n",
    "        (df['lat'].between(-11, 6)) &\n",
    "        (df['long'].between(95, 141))\n",
    "    )\n",
    "    invalid_count = (~mask_valid).sum()\n",
    "    if invalid_count > 0:\n",
    "        print(f'âš ï¸  Found {invalid_count} rows with coordinates outside Indonesia range')\n",
    "        df['coord_invalid'] = (~mask_valid).astype(int)\n",
    "    else:\n",
    "        df['coord_invalid'] = 0\n",
    "    \n",
    "    # Konversi ke radian\n",
    "    df['lat_rad']  = np.radians(df['lat'])\n",
    "    df['long_rad'] = np.radians(df['long'])\n",
    "    \n",
    "    print('âœ… Coordinate features created: lat_rad, long_rad, coord_invalid')\n",
    "    return df\n",
    "\n",
    "df = transform_coordinates(df)\n",
    "df[['lat','long','lat_rad','long_rad']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3_md",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”µ STEP 3: DBSCAN Spatial Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbscan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dbscan_spatial(\n",
    "    df: pd.DataFrame,\n",
    "    jenis: str,\n",
    "    eps_km: float = 0.1,\n",
    "    min_samples: int = 3\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Jalankan DBSCAN spatial per SKPD untuk jenis absensi tertentu (M/P).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eps_km      : radius cluster dalam km (default 100m)\n",
    "    min_samples : minimum titik untuk membentuk cluster\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Series dengan label cluster per baris\n",
    "    \"\"\"\n",
    "    # Radius bumi dalam km\n",
    "    EARTH_RADIUS_KM = 6371.0\n",
    "    eps_rad = eps_km / EARTH_RADIUS_KM\n",
    "    \n",
    "    result = pd.Series(-1, index=df.index, dtype=int)\n",
    "    \n",
    "    subset = df[df['jenis'] == jenis].copy()\n",
    "    \n",
    "    for skpd_id, group in subset.groupby('id_skpd'):\n",
    "        if len(group) < min_samples:\n",
    "            continue\n",
    "        \n",
    "        coords = group[['lat_rad', 'long_rad']].values\n",
    "        \n",
    "        db = DBSCAN(\n",
    "            eps=eps_rad,\n",
    "            min_samples=min_samples,\n",
    "            algorithm='ball_tree',\n",
    "            metric='haversine'\n",
    "        ).fit(coords)\n",
    "        \n",
    "        # Offset label agar tidak overlap antar SKPD\n",
    "        labels = db.labels_\n",
    "        max_label = result.max() + 1\n",
    "        labels_offset = np.where(labels >= 0, labels + max_label, -1)\n",
    "        \n",
    "        result.loc[group.index] = labels_offset\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Konfigurasi DBSCAN - sesuaikan dengan kebutuhan\n",
    "# ============================================================\n",
    "EPS_KM      = 0.1   # 100 meter radius\n",
    "MIN_SAMPLES = 3     # minimal 3 titik\n",
    "\n",
    "print('ðŸ”„ Running DBSCAN for MASUK...')\n",
    "df['cluster_masuk'] = run_dbscan_spatial(df, 'M', EPS_KM, MIN_SAMPLES)\n",
    "\n",
    "print('ðŸ”„ Running DBSCAN for PULANG...')\n",
    "df['cluster_pulang'] = run_dbscan_spatial(df, 'P', EPS_KM, MIN_SAMPLES)\n",
    "\n",
    "# Noise flags\n",
    "df['is_noise_masuk']  = (df['cluster_masuk']  == -1).astype(int)\n",
    "df['is_noise_pulang'] = (df['cluster_pulang'] == -1).astype(int)\n",
    "\n",
    "# Cluster size\n",
    "cluster_size_masuk  = df[df['jenis']=='M'].groupby('cluster_masuk')['karyawan_id'].transform('count')\n",
    "cluster_size_pulang = df[df['jenis']=='P'].groupby('cluster_pulang')['karyawan_id'].transform('count')\n",
    "\n",
    "df['cluster_size_masuk']  = cluster_size_masuk\n",
    "df['cluster_size_pulang'] = cluster_size_pulang\n",
    "df[['cluster_size_masuk','cluster_size_pulang']] = df[['cluster_size_masuk','cluster_size_pulang']].fillna(0).astype(int)\n",
    "\n",
    "print(f'\\nâœ… DBSCAN done')\n",
    "print(f\"Cluster MASUK  : {df['cluster_masuk'].nunique() - 1} clusters, {df['is_noise_masuk'].sum()} noise\")\n",
    "print(f\"Cluster PULANG : {df['cluster_pulang'].nunique() - 1} clusters, {df['is_noise_pulang'].sum()} noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4_md",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”´ STEP 4: Estimasi Centroid Kantor per SKPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centroid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_office_location(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Estimasi lokasi kantor per SKPD:\n",
    "    1. Ambil data MASUK saja\n",
    "    2. Cari cluster terbesar per SKPD\n",
    "    3. Hitung centroid (rata-rata lat/long) dari cluster terbesar\n",
    "    4. Simpan sebagai office_lat, office_long\n",
    "    \"\"\"\n",
    "    office_locations = {}\n",
    "    \n",
    "    masuk_df = df[df['jenis'] == 'M'].copy()\n",
    "    \n",
    "    for skpd_id, group in masuk_df.groupby('id_skpd'):\n",
    "        # Filter bukan noise\n",
    "        non_noise = group[group['cluster_masuk'] != -1]\n",
    "        \n",
    "        if len(non_noise) == 0:\n",
    "            # Fallback: gunakan median semua titik\n",
    "            office_locations[skpd_id] = {\n",
    "                'office_lat':  group['lat'].median(),\n",
    "                'office_long': group['long'].median()\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        # Cari cluster terbesar\n",
    "        cluster_counts = non_noise['cluster_masuk'].value_counts()\n",
    "        largest_cluster = cluster_counts.idxmax()\n",
    "        \n",
    "        # Centroid cluster terbesar\n",
    "        cluster_points = non_noise[non_noise['cluster_masuk'] == largest_cluster]\n",
    "        office_locations[skpd_id] = {\n",
    "            'office_lat':  cluster_points['lat'].mean(),\n",
    "            'office_long': cluster_points['long'].mean()\n",
    "        }\n",
    "    \n",
    "    # Mapping ke dataframe\n",
    "    # Gunakan from_dict orient='index' agar tidak ada masalah tipe data\n",
    "    office_df = pd.DataFrame.from_dict(office_locations, orient='index').reset_index()\n",
    "    office_df.columns = ['id_skpd', 'office_lat', 'office_long']\n",
    "    \n",
    "    # Samakan tipe id_skpd agar merge tidak gagal\n",
    "    office_df['id_skpd'] = office_df['id_skpd'].astype(df['id_skpd'].dtype)\n",
    "    \n",
    "    df = df.merge(office_df, on='id_skpd', how='left')\n",
    "    \n",
    "    print(f'âœ… Office locations estimated for {len(office_locations)} SKPDs')\n",
    "    print(office_df.head())\n",
    "    return df\n",
    "\n",
    "df = estimate_office_location(df)\n",
    "df[['id_skpd','office_lat','office_long']].drop_duplicates().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5_md",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”´ STEP 5: Haversine Distance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "haversine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Hitung jarak Haversine antara dua titik koordinat.\n",
    "    Return: jarak dalam kilometer\n",
    "    \"\"\"\n",
    "    R = 6371.0  # Radius bumi km\n",
    "    \n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "\n",
    "def calculate_distances(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Hitung jarak setiap absensi ke kantor SKPD-nya.\n",
    "    Tambahkan flag:\n",
    "    - outside_300m : jarak > 300m\n",
    "    - very_far     : jarak > 5km\n",
    "    - extreme_far  : jarak > 50km (indikasi fake GPS)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['dist_km'] = haversine_distance(\n",
    "        df['lat'].values,\n",
    "        df['long'].values,\n",
    "        df['office_lat'].values,\n",
    "        df['office_long'].values\n",
    "    )\n",
    "    \n",
    "    df['outside_300m'] = (df['dist_km'] > 0.3).astype(int)\n",
    "    df['very_far']     = (df['dist_km'] > 5.0).astype(int)\n",
    "    df['extreme_far']  = (df['dist_km'] > 50.0).astype(int)\n",
    "    \n",
    "    print('âœ… Distance features created: dist_km, outside_300m, very_far, extreme_far')\n",
    "    print(f\"   Outside 300m : {df['outside_300m'].sum()} ({df['outside_300m'].mean()*100:.1f}%)\")\n",
    "    print(f\"   Very far >5km: {df['very_far'].sum()} ({df['very_far'].mean()*100:.1f}%)\")\n",
    "    print(f\"   Extreme >50km: {df['extreme_far'].sum()} ({df['extreme_far'].mean()*100:.1f}%)\")\n",
    "    return df\n",
    "\n",
    "df = calculate_distances(df)\n",
    "df[['karyawan_id','jenis','dist_km','outside_300m','very_far','extreme_far']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6_md",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŸ¤ STEP 6: Validation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Buat feature validasi:\n",
    "    - no_note         : tidak ada catatan/alasan\n",
    "    - far_no_note     : jauh + tidak ada catatan (fraud kuat)\n",
    "    - far_with_note   : jauh tapi ada catatan (tugas luar valid)\n",
    "    - near_but_status0: dekat tapi sistem lama bilang di luar (mismatch)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Cek catatan\n",
    "    df['no_note'] = df['catatan'].isna().astype(int)\n",
    "    \n",
    "    # Kombinasi jarak + catatan\n",
    "    df['far_no_note']   = ((df['outside_300m'] == 1) & (df['no_note'] == 1)).astype(int)\n",
    "    df['far_with_note'] = ((df['outside_300m'] == 1) & (df['no_note'] == 0)).astype(int)\n",
    "    \n",
    "    # Mismatch: dekat secara GPS tapi sistem lama bilang di luar\n",
    "    df['near_but_status0'] = (\n",
    "        (df['outside_300m'] == 0) & \n",
    "        (df['status_lokasi'] == 0)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Mismatch sebaliknya: jauh tapi sistem lama bilang di dalam\n",
    "    df['far_but_status1'] = (\n",
    "        (df['outside_300m'] == 1) & \n",
    "        (df['status_lokasi'] == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    print('âœ… Validation features created: no_note, far_no_note, far_with_note, near_but_status0, far_but_status1')\n",
    "    print(f\"   far_no_note     : {df['far_no_note'].sum()}\")\n",
    "    print(f\"   far_with_note   : {df['far_with_note'].sum()}\")\n",
    "    print(f\"   near_but_status0: {df['near_but_status0'].sum()}\")\n",
    "    print(f\"   far_but_status1 : {df['far_but_status1'].sum()}\")\n",
    "    return df\n",
    "\n",
    "df = create_validation_features(df)\n",
    "df[['karyawan_id','dist_km','no_note','far_no_note','far_with_note','near_but_status0']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7_md",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŸ£ STEP 7: ST-DBSCAN (Spatio-Temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stdbscan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_st_dbscan(\n",
    "    df: pd.DataFrame,\n",
    "    jenis: str,\n",
    "    eps_km: float = 0.1,\n",
    "    eps_time_hours: float = 1.0,\n",
    "    min_samples: int = 3\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    ST-DBSCAN: DBSCAN dengan dimensi spasial + temporal.\n",
    "    \n",
    "    Menggunakan normalisasi:\n",
    "    - Koordinat: lat/long dalam derajat\n",
    "    - Waktu: jam_desimal (0-24)\n",
    "    \n",
    "    Jarak gabungan menggunakan Euclidean setelah normalisasi.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eps_km         : radius spasial dalam km\n",
    "    eps_time_hours : radius temporal dalam jam\n",
    "    min_samples    : minimum titik\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    result = pd.Series(-1, index=df.index, dtype=int)\n",
    "    subset = df[df['jenis'] == jenis].copy()\n",
    "    \n",
    "    # Konversi eps ke unit yang sama\n",
    "    # 1 derajat lat â‰ˆ 111 km\n",
    "    eps_deg = eps_km / 111.0\n",
    "    \n",
    "    for skpd_id, group in subset.groupby('id_skpd'):\n",
    "        if len(group) < min_samples:\n",
    "            continue\n",
    "        \n",
    "        # Feature: lat, long, jam_desimal\n",
    "        features = group[['lat', 'long', 'jam_desimal']].copy()\n",
    "        \n",
    "        # Normalisasi: skala waktu agar sebanding dengan spasial\n",
    "        # eps_time_hours jam = eps_deg derajat (secara proporsional)\n",
    "        time_scale = eps_deg / eps_time_hours\n",
    "        features['jam_desimal_scaled'] = features['jam_desimal'] * time_scale\n",
    "        \n",
    "        X = features[['lat', 'long', 'jam_desimal_scaled']].values\n",
    "        \n",
    "        db = DBSCAN(\n",
    "            eps=eps_deg,\n",
    "            min_samples=min_samples,\n",
    "            algorithm='auto',\n",
    "            metric='euclidean'\n",
    "        ).fit(X)\n",
    "        \n",
    "        labels = db.labels_\n",
    "        max_label = result.max() + 1\n",
    "        labels_offset = np.where(labels >= 0, labels + max_label, -1)\n",
    "        result.loc[group.index] = labels_offset\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Konfigurasi ST-DBSCAN\n",
    "# ============================================================\n",
    "ST_EPS_KM    = 0.1   # 100 meter\n",
    "ST_EPS_HOURS = 1.0   # 1 jam\n",
    "ST_MIN_SAMP  = 3\n",
    "\n",
    "print('ðŸ”„ Running ST-DBSCAN for MASUK...')\n",
    "df['st_cluster_masuk'] = run_st_dbscan(df, 'M', ST_EPS_KM, ST_EPS_HOURS, ST_MIN_SAMP)\n",
    "\n",
    "print('ðŸ”„ Running ST-DBSCAN for PULANG...')\n",
    "df['st_cluster_pulang'] = run_st_dbscan(df, 'P', ST_EPS_KM, ST_EPS_HOURS, ST_MIN_SAMP)\n",
    "\n",
    "df['is_st_noise_masuk']  = (df['st_cluster_masuk']  == -1).astype(int)\n",
    "df['is_st_noise_pulang'] = (df['st_cluster_pulang'] == -1).astype(int)\n",
    "\n",
    "print(f'\\nâœ… ST-DBSCAN done')\n",
    "print(f\"ST Noise MASUK : {df['is_st_noise_masuk'].sum()}\")\n",
    "print(f\"ST Noise PULANG: {df['is_st_noise_pulang'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8_md",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŸ¢ STEP 8: Anomaly Scoring & Risk Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_anomaly_score(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Hitung anomaly_score berdasarkan kombinasi sinyal:\n",
    "    \n",
    "    Sinyal & Bobot:\n",
    "    +3 : extreme_far (>50km, fake GPS kuat)\n",
    "    +2 : very_far (>5km)\n",
    "    +2 : far_no_note (jauh + tidak ada alasan)\n",
    "    +1 : outside_300m (di luar 300m)\n",
    "    +1 : is_noise_masuk (tidak masuk cluster manapun)\n",
    "    +1 : is_st_noise_masuk (tidak konsisten waktu+lokasi)\n",
    "    +1 : far_but_status1 (mismatch sistem lama)\n",
    "    -1 : far_with_note (ada alasan, kurangi skor)\n",
    "    \n",
    "    Risk Level:\n",
    "    - LOW  : score 0-1\n",
    "    - MED  : score 2-3\n",
    "    - HIGH : score >= 4\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    score = pd.Series(0, index=df.index)\n",
    "    \n",
    "    # Bobot sinyal\n",
    "    score += df.get('extreme_far',      pd.Series(0, index=df.index)) * 3\n",
    "    score += df.get('very_far',         pd.Series(0, index=df.index)) * 2\n",
    "    score += df.get('far_no_note',      pd.Series(0, index=df.index)) * 2\n",
    "    score += df.get('outside_300m',     pd.Series(0, index=df.index)) * 1\n",
    "    score += df.get('is_noise_masuk',   pd.Series(0, index=df.index)) * 1\n",
    "    score += df.get('is_st_noise_masuk',pd.Series(0, index=df.index)) * 1\n",
    "    score += df.get('far_but_status1',  pd.Series(0, index=df.index)) * 1\n",
    "    score -= df.get('far_with_note',    pd.Series(0, index=df.index)) * 1\n",
    "    \n",
    "    # Pastikan tidak negatif\n",
    "    score = score.clip(lower=0)\n",
    "    \n",
    "    df['anomaly_score'] = score.astype(int)\n",
    "    \n",
    "    # Risk Level\n",
    "    def score_to_risk(s):\n",
    "        if s >= 4:\n",
    "            return 'HIGH'\n",
    "        elif s >= 2:\n",
    "            return 'MED'\n",
    "        else:\n",
    "            return 'LOW'\n",
    "    \n",
    "    df['risk_level'] = df['anomaly_score'].apply(score_to_risk)\n",
    "    \n",
    "    print('âœ… Anomaly scoring done')\n",
    "    print('\\nDistribusi Risk Level:')\n",
    "    print(df['risk_level'].value_counts())\n",
    "    print('\\nDistribusi Anomaly Score:')\n",
    "    print(df['anomaly_score'].value_counts().sort_index())\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = calculate_anomaly_score(df)\n",
    "df[['karyawan_id','jenis','dist_km','anomaly_score','risk_level']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9_md",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ’¾ STEP 9: Export Hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export ke CSV untuk dipakai di Streamlit\n",
    "OUTPUT_PATH = 'absensi_processed.csv'\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f'âœ… Exported to {OUTPUT_PATH}')\n",
    "print(f'   Shape: {df.shape}')\n",
    "print(f'   Columns: {df.columns.tolist()}')\n",
    "\n",
    "# Summary statistik\n",
    "print('\\n=== SUMMARY ===')\n",
    "print(f\"Total absensi    : {len(df)}\")\n",
    "print(f\"Total karyawan   : {df['karyawan_id'].nunique()}\")\n",
    "print(f\"Total SKPD       : {df['id_skpd'].nunique()}\")\n",
    "print(f\"HIGH risk        : {(df['risk_level']=='HIGH').sum()} ({(df['risk_level']=='HIGH').mean()*100:.1f}%)\")\n",
    "print(f\"MED risk         : {(df['risk_level']=='MED').sum()} ({(df['risk_level']=='MED').mean()*100:.1f}%)\")\n",
    "print(f\"LOW risk         : {(df['risk_level']=='LOW').sum()} ({(df['risk_level']=='LOW').mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_md",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ—ºï¸ Quick Visualization (Folium) - Preview di Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "folium_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "\n",
    "def create_preview_map(df: pd.DataFrame, max_points: int = 500) -> folium.Map:\n",
    "    \"\"\"\n",
    "    Buat peta preview dengan Folium.\n",
    "    Warna marker berdasarkan risk_level:\n",
    "    - HIGH : merah\n",
    "    - MED  : oranye\n",
    "    - LOW  : hijau\n",
    "    \"\"\"\n",
    "    color_map = {'HIGH': 'red', 'MED': 'orange', 'LOW': 'green'}\n",
    "    \n",
    "    # Center peta\n",
    "    center_lat  = df['lat'].median()\n",
    "    center_long = df['long'].median()\n",
    "    \n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_long],\n",
    "        zoom_start=12,\n",
    "        tiles='OpenStreetMap'\n",
    "    )\n",
    "    \n",
    "    # Sample data agar tidak terlalu berat\n",
    "    sample = df.sample(min(max_points, len(df)), random_state=42)\n",
    "    \n",
    "    # Marker cluster\n",
    "    mc = MarkerCluster().add_to(m)\n",
    "    \n",
    "    for _, row in sample.iterrows():\n",
    "        color = color_map.get(row.get('risk_level', 'LOW'), 'blue')\n",
    "        popup_text = f\"\"\"\n",
    "        <b>Karyawan:</b> {row['karyawan_id']}<br>\n",
    "        <b>SKPD:</b> {row['id_skpd']}<br>\n",
    "        <b>Jenis:</b> {row['jenis']}<br>\n",
    "        <b>Waktu:</b> {row['tanggal_kirim']}<br>\n",
    "        <b>Jarak ke kantor:</b> {row.get('dist_km', 0):.3f} km<br>\n",
    "        <b>Risk Level:</b> <span style='color:{color}'><b>{row.get('risk_level','N/A')}</b></span><br>\n",
    "        <b>Anomaly Score:</b> {row.get('anomaly_score', 0)}\n",
    "        \"\"\"\n",
    "        folium.CircleMarker(\n",
    "            location=[row['lat'], row['long']],\n",
    "            radius=6,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_opacity=0.7,\n",
    "            popup=folium.Popup(popup_text, max_width=250)\n",
    "        ).add_to(mc)\n",
    "    \n",
    "    # Tambahkan marker kantor\n",
    "    offices = df[['id_skpd','office_lat','office_long']].drop_duplicates()\n",
    "    for _, row in offices.iterrows():\n",
    "        if pd.notna(row['office_lat']):\n",
    "            folium.Marker(\n",
    "                location=[row['office_lat'], row['office_long']],\n",
    "                popup=f\"Kantor SKPD {row['id_skpd']}\",\n",
    "                icon=folium.Icon(color='blue', icon='building', prefix='fa')\n",
    "            ).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "preview_map = create_preview_map(df)\n",
    "preview_map  # Tampilkan di Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heatmap_md",
   "metadata": {},
   "source": [
    "### ðŸ”¥ Heatmap Anomali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap khusus HIGH risk\n",
    "high_risk = df[df['risk_level'] == 'HIGH']\n",
    "\n",
    "center_lat  = df['lat'].median()\n",
    "center_long = df['long'].median()\n",
    "\n",
    "heatmap = folium.Map(\n",
    "    location=[center_lat, center_long],\n",
    "    zoom_start=12\n",
    ")\n",
    "\n",
    "heat_data = high_risk[['lat','long']].dropna().values.tolist()\n",
    "HeatMap(heat_data, radius=15, blur=10, max_zoom=13).add_to(heatmap)\n",
    "\n",
    "print(f'Heatmap dari {len(heat_data)} titik HIGH risk')\n",
    "heatmap"
   ]
  }
 ]
}
